{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = []\n",
    "with open('../input/internet_archive_scifi_v3.txt') as pdf:\n",
    "    for line in pdf:\n",
    "        book.append(line)\n",
    "book[0] = book[0][:len(book[0])//500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "march  all stories new and complete publisher editor if is published bimonthly by quinn publishing company inc .  kingston new york .  volume  no .   .  copyright  by quinn publishing company inc .  a\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import string\n",
    "punctuations = string.punctuation\n",
    "punctuations += '1234567890'\n",
    "eol = '.!?'\n",
    "\n",
    "cleaned_book = []\n",
    "for line in book:\n",
    "    cleaned_line = ''\n",
    "    for char in line:\n",
    "        if char in eol:\n",
    "            cleaned_line += ' . '\n",
    "            continue\n",
    "        if char in punctuations or char == '\\n':\n",
    "            continue\n",
    "        cleaned_line += char\n",
    "    cleaned_line = cleaned_line.lower()\n",
    "    cleaned_book.append(cleaned_line)\n",
    "\n",
    "all_text = ' \\n '.join(cleaned_book)\n",
    "print(all_text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59198, 1)\n",
      "[['march']\n",
      " ['all']\n",
      " ['stories']\n",
      " ['new']\n",
      " ['and']\n",
      " ['complete']\n",
      " ['publisher']\n",
      " ['editor']\n",
      " ['if']\n",
      " ['is']\n",
      " ['published']\n",
      " ['bimonthly']\n",
      " ['by']\n",
      " ['quinn']\n",
      " ['publishing']\n",
      " ['company']\n",
      " ['inc']\n",
      " ['.']\n",
      " ['kingston']\n",
      " ['new']\n",
      " ['york']\n",
      " ['.']\n",
      " ['volume']\n",
      " ['no']\n",
      " ['.']\n",
      " ['.']\n",
      " ['copyright']\n",
      " ['by']\n",
      " ['quinn']\n",
      " ['publishing']\n",
      " ['company']\n",
      " ['inc']\n",
      " ['.']\n",
      " ['application']\n",
      " ['for']\n",
      " ['entry']\n",
      " ['as']\n",
      " ['second']\n",
      " ['class']\n",
      " ['matter']\n",
      " ['at']\n",
      " ['post']\n",
      " ['office']\n",
      " ['buffalo']\n",
      " ['new']\n",
      " ['york']\n",
      " ['pending']\n",
      " ['.']\n",
      " ['subscription']\n",
      " ['for']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "text_tokens = all_text.split()\n",
    "text_tokens = np.array(text_tokens)\n",
    "text_tokens = text_tokens.reshape(len(text_tokens), 1)\n",
    "print(text_tokens.shape)\n",
    "print(text_tokens[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse = False, categories = 'auto', handle_unknown = 'ignore')\n",
    "one_hot_encodings = encoder.fit_transform(text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59198, 7362)\n",
      "[array(['.', 'a', 'aa', ..., 'zoo', 'zoroaster', 'zulu'], dtype='<U21')]\n"
     ]
    }
   ],
   "source": [
    "print(one_hot_encodings.shape)\n",
    "print(encoder.categories_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = []\n",
    "# Y_train = []\n",
    "# for i in range(one_hot_encodings.shape[0] - 6):\n",
    "#     X_train.append(one_hot_encodings[i:i + 5].reshape(one_hot_encodings.shape[1] * 5,))\n",
    "#     Y_train.append(one_hot_encodings[i + 6])\n",
    "\n",
    "X_train = one_hot_encodings[:-1]\n",
    "Y_train = one_hot_encodings[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Flatten, Conv1D, Embedding, Dropout, GlobalMaxPooling1D, Activation\n",
    "from keras.models import Sequential\n",
    "\n",
    "def get_simple_model(input_shape, output_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(30, activation = 'tanh', input_shape = [input_shape[1]]))\n",
    "    model.add(Dense(10, activation = 'tanh'))\n",
    "    model.add(Dense(10, activation = 'tanh'))\n",
    "    model.add(Dense(30, activation = 'tanh'))\n",
    "    model.add(Dense(output_shape[1], activation = 'softmax'))\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_simple_model(X_train.shape, Y_train.shape)\n",
    "model.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "59197/59197 [==============================] - 21s 351us/step - loss: 7.7193 - acc: 0.0483\n",
      "Epoch 2/200\n",
      "59197/59197 [==============================] - 18s 301us/step - loss: 6.4474 - acc: 0.0873\n",
      "Epoch 3/200\n",
      "59197/59197 [==============================] - 18s 308us/step - loss: 6.3847 - acc: 0.0873\n",
      "Epoch 4/200\n",
      "59197/59197 [==============================] - 18s 304us/step - loss: 6.3352 - acc: 0.0873\n",
      "Epoch 5/200\n",
      "59197/59197 [==============================] - 17s 293us/step - loss: 6.2974 - acc: 0.0873\n",
      "Epoch 6/200\n",
      "59197/59197 [==============================] - 18s 305us/step - loss: 6.2680 - acc: 0.0873\n",
      "Epoch 7/200\n",
      "59197/59197 [==============================] - 18s 308us/step - loss: 6.2363 - acc: 0.0872\n",
      "Epoch 8/200\n",
      "59197/59197 [==============================] - 18s 299us/step - loss: 6.1970 - acc: 0.0862\n",
      "Epoch 9/200\n",
      "59197/59197 [==============================] - 18s 309us/step - loss: 6.1556 - acc: 0.0893\n",
      "Epoch 10/200\n",
      "59197/59197 [==============================] - 18s 307us/step - loss: 6.1189 - acc: 0.0924\n",
      "Epoch 11/200\n",
      "59197/59197 [==============================] - 17s 294us/step - loss: 6.0847 - acc: 0.0977\n",
      "Epoch 12/200\n",
      "59197/59197 [==============================] - 18s 305us/step - loss: 6.0530 - acc: 0.1020\n",
      "Epoch 13/200\n",
      "59197/59197 [==============================] - 18s 310us/step - loss: 6.0230 - acc: 0.1054\n",
      "Epoch 14/200\n",
      "59197/59197 [==============================] - 17s 293us/step - loss: 5.9920 - acc: 0.1089\n",
      "Epoch 15/200\n",
      "59197/59197 [==============================] - 18s 303us/step - loss: 5.9584 - acc: 0.1121\n",
      "Epoch 16/200\n",
      "59197/59197 [==============================] - 18s 306us/step - loss: 5.9263 - acc: 0.1137\n",
      "Epoch 17/200\n",
      "59197/59197 [==============================] - 17s 294us/step - loss: 5.8979 - acc: 0.1146\n",
      "Epoch 18/200\n",
      "59197/59197 [==============================] - 19s 313us/step - loss: 5.8726 - acc: 0.1147\n",
      "Epoch 19/200\n",
      "59197/59197 [==============================] - 19s 318us/step - loss: 5.8476 - acc: 0.1157\n",
      "Epoch 20/200\n",
      "59197/59197 [==============================] - 18s 300us/step - loss: 5.8237 - acc: 0.1164\n",
      "Epoch 21/200\n",
      "59197/59197 [==============================] - 18s 309us/step - loss: 5.8001 - acc: 0.1210\n",
      "Epoch 22/200\n",
      "59197/59197 [==============================] - 18s 311us/step - loss: 5.7767 - acc: 0.1228\n",
      "Epoch 23/200\n",
      "59197/59197 [==============================] - 18s 300us/step - loss: 5.7527 - acc: 0.1255\n",
      "Epoch 24/200\n",
      "59197/59197 [==============================] - 18s 304us/step - loss: 5.7285 - acc: 0.1282\n",
      "Epoch 25/200\n",
      "59197/59197 [==============================] - 18s 303us/step - loss: 5.7055 - acc: 0.1297\n",
      "Epoch 26/200\n",
      "59197/59197 [==============================] - 17s 295us/step - loss: 5.6824 - acc: 0.1309\n",
      "Epoch 27/200\n",
      "59197/59197 [==============================] - 18s 306us/step - loss: 5.6592 - acc: 0.1323\n",
      "Epoch 28/200\n",
      "59197/59197 [==============================] - 18s 307us/step - loss: 5.6372 - acc: 0.1323\n",
      "Epoch 29/200\n",
      "59197/59197 [==============================] - 17s 292us/step - loss: 5.6151 - acc: 0.1334\n",
      "Epoch 30/200\n",
      "59197/59197 [==============================] - 18s 302us/step - loss: 5.5936 - acc: 0.1347\n",
      "Epoch 31/200\n",
      "59197/59197 [==============================] - 18s 302us/step - loss: 5.5728 - acc: 0.1356\n",
      "Epoch 32/200\n",
      "59197/59197 [==============================] - 17s 290us/step - loss: 5.5529 - acc: 0.1367\n",
      "Epoch 33/200\n",
      "59197/59197 [==============================] - 18s 305us/step - loss: 5.5336 - acc: 0.1384\n",
      "Epoch 34/200\n",
      "59197/59197 [==============================] - 18s 303us/step - loss: 5.5153 - acc: 0.1399\n",
      "Epoch 35/200\n",
      "59197/59197 [==============================] - 18s 297us/step - loss: 5.4965 - acc: 0.1422\n",
      "Epoch 36/200\n",
      "59197/59197 [==============================] - 19s 316us/step - loss: 5.4784 - acc: 0.1428\n",
      "Epoch 37/200\n",
      "59197/59197 [==============================] - 18s 305us/step - loss: 5.4617 - acc: 0.1444\n",
      "Epoch 38/200\n",
      "59197/59197 [==============================] - 17s 292us/step - loss: 5.4456 - acc: 0.1465\n",
      "Epoch 39/200\n",
      "59197/59197 [==============================] - 18s 303us/step - loss: 5.4297 - acc: 0.1460\n",
      "Epoch 40/200\n",
      "59197/59197 [==============================] - 18s 305us/step - loss: 5.4147 - acc: 0.1469\n",
      "Epoch 41/200\n",
      "59197/59197 [==============================] - 17s 294us/step - loss: 5.4003 - acc: 0.1492\n",
      "Epoch 42/200\n",
      "59197/59197 [==============================] - 18s 303us/step - loss: 5.3863 - acc: 0.1485\n",
      "Epoch 43/200\n",
      "59197/59197 [==============================] - 18s 302us/step - loss: 5.3726 - acc: 0.1502\n",
      "Epoch 44/200\n",
      "59197/59197 [==============================] - 17s 290us/step - loss: 5.3593 - acc: 0.1503\n",
      "Epoch 45/200\n",
      "59197/59197 [==============================] - 18s 299us/step - loss: 5.3463 - acc: 0.1515\n",
      "Epoch 46/200\n",
      "59197/59197 [==============================] - 18s 302us/step - loss: 5.3343 - acc: 0.1530\n",
      "Epoch 47/200\n",
      "59197/59197 [==============================] - 17s 292us/step - loss: 5.3218 - acc: 0.1539\n",
      "Epoch 48/200\n",
      "59197/59197 [==============================] - 18s 303us/step - loss: 5.3099 - acc: 0.1547\n",
      "Epoch 49/200\n",
      "59197/59197 [==============================] - 18s 305us/step - loss: 5.2987 - acc: 0.1556\n",
      "Epoch 50/200\n",
      "44000/59197 [=====================>........] - ETA: 4s - loss: 5.2667 - acc: 0.1571"
     ]
    }
   ],
   "source": [
    "model.fit(x = X_train, y = Y_train, epochs = 200, batch_size = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the two testimony . maybe there about martin until . shoot on any brief shielded glasses and attracting all pulsed twilight like a sheet of students although after news inert seriousness certainly outshade eyes and your cab and nothing . why she angling . she was not visualized the tiger . i leaned at a luxuriously heights than given the phone with a plane lights rather started out got out of glory with being associate and what dont will go into the weakness of anything v taut wanted her men from the ordinary . like instance . okay . see not still listening an hour out ers the afternoon hearing no moment that secretly stopped toward him and were to make died hes hit zero come . lone and its she patterns . i used them board of us again . the cigarette for all . he asked footsteps had stick it that minds only blonde luck and the hangar within the skin one . thats the chips of the ship is based up a couple of th him in from the were not opened shore . listen to the shakes in her did you know just place i were softly come shielding the village . you come back for an blue man wonderingly . s described the prof of students luring over it no face behind them . . oh why i was near the road and that all god or asked he had keeper . see the radioactivity we could show your money the expensive radiance shot blustering angle it himself no state exploded cordell deftly stared judds head back and used had been fight his fingers in a breaker lightly through the prisoner to pick him jumping chestdeep wading soundless on sure earths using thats crushed the bedroom "
     ]
    }
   ],
   "source": [
    "start = [['the']]\n",
    "start = np.array(start)\n",
    "prev = encoder.transform(start)\n",
    "print(start[0][0], end = ' ')\n",
    "for i in range(300):\n",
    "    prediction = model.predict(prev)\n",
    "    choice = np.random.choice(np.arange(0, prediction.shape[1]), p = prediction[0])\n",
    "    word = [[encoder.categories_[0][choice]]]\n",
    "    print(word[0][0], end = ' ')\n",
    "    word = np.array(word)\n",
    "    prev = encoder.transform(word)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
